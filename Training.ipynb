{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c1aa0c-e749-4425-8df0-791db55441ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T12:49:06.766916Z",
     "start_time": "2024-08-03T12:49:03.610434Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/satvikahuja13/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "from data_loader import get_loader\n",
    "from data_loader_val import get_loader as val_loader\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from nlp_utils import clean_sentence, bleu_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb1b827-65de-46d5-bf83-df4ec98ddf3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T12:49:06.813341Z",
     "start_time": "2024-08-03T12:49:06.766163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['.DS_Store', 'images', 'annotations']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cocoapi_dir = r\"cocoapi/\"\n",
    "folders = [folder for folder in os.listdir(\"cocoapi/\")]\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05f818ad-82f0-4cfe-9444-1627ccc69007",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T12:49:06.814430Z",
     "start_time": "2024-08-03T12:49:06.788156Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "vocab_threshold = 5\n",
    "vocab_from_file = True\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_epochs = 3\n",
    "save_every = 1\n",
    "print_every = 20\n",
    "log_file = \"training_log.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ffeb9b-64e9-45a9-86ca-52c177a0d3e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T12:49:06.829147Z",
     "start_time": "2024-08-03T12:49:06.808159Z"
    }
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "     transforms.Resize(256),\n",
    "     transforms.RandomCrop(224),\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(\n",
    "         (0.485, 0.456, 0.406),\n",
    "         (0.229, 0.224, 0.225),\n",
    "     ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd4b7628-36b7-417d-a6f6-3c17d064aece",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T12:49:36.863570Z",
     "start_time": "2024-08-03T12:49:06.828491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.49s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 591753/591753 [00:29<00:00, 20382.69it/s]\n"
     ]
    }
   ],
   "source": [
    "data_loader = get_loader(\n",
    "    transform=transform_train,\n",
    "    mode=\"train\",\n",
    "    batch_size=batch_size,\n",
    "    vocab_threshold=vocab_threshold,\n",
    "    vocab_from_file=vocab_from_file,\n",
    "    cocoapi_loc = cocoapi_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf4f4e8a-4b4f-45e0-ab2f-22bd49eaf9bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T12:49:37.466977Z",
     "start_time": "2024-08-03T12:49:36.864571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is : 11543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satvikahuja13/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/satvikahuja13/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "print(\"vocab size is :\", vocab_size)\n",
    "#initializing the encoder and decoder\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "#move models to device\n",
    "device = torch.device(\"mps\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "#defining the loss function\n",
    "criterion = (\n",
    "             nn.CrossEntropyLoss().to(device)\n",
    ")\n",
    "\n",
    "#specifying the learnable parameters of the mode\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "#Defining the optimize\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "#Set the total number of training steps per epoc\n",
    "total_step = math.ceil(len(data_loader.dataset)/data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f52dbb1-b325-4f85-8a37-e14b457cd83e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T12:49:37.487086Z",
     "start_time": "2024-08-03T12:49:37.467247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4624\n"
     ]
    }
   ],
   "source": [
    "print(total_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7786c407-38b0-4be3-a661-d0bd7c1c2db0",
   "metadata": {},
   "source": [
    "## **Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a13a9c0-03a4-45a0-94d0-f4227421889d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T18:01:23.376515Z",
     "start_time": "2024-08-03T12:49:37.488187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [20/4624], Loss: 5.1296, Perplexity: 168.9420\n",
      "Epoch [1/3], Step [40/4624], Loss: 4.3825, Perplexity: 80.0355\n",
      "Epoch [1/3], Step [60/4624], Loss: 4.0272, Perplexity: 56.1009\n",
      "Epoch [1/3], Step [80/4624], Loss: 4.6615, Perplexity: 105.7949\n",
      "Epoch [1/3], Step [100/4624], Loss: 3.9281, Perplexity: 50.8078\n",
      "Epoch [1/3], Step [120/4624], Loss: 3.7151, Perplexity: 41.0629\n",
      "Epoch [1/3], Step [140/4624], Loss: 3.5556, Perplexity: 35.0083\n",
      "Epoch [1/3], Step [160/4624], Loss: 3.6330, Perplexity: 37.8246\n",
      "Epoch [1/3], Step [180/4624], Loss: 3.5242, Perplexity: 33.9250\n",
      "Epoch [1/3], Step [200/4624], Loss: 3.6510, Perplexity: 38.5131\n",
      "Epoch [1/3], Step [220/4624], Loss: 3.5533, Perplexity: 34.9273\n",
      "Epoch [1/3], Step [240/4624], Loss: 3.5534, Perplexity: 34.9325\n",
      "Epoch [1/3], Step [260/4624], Loss: 3.2923, Perplexity: 26.9043\n",
      "Epoch [1/3], Step [280/4624], Loss: 3.3987, Perplexity: 29.9254\n",
      "Epoch [1/3], Step [300/4624], Loss: 3.2355, Perplexity: 25.4185\n",
      "Epoch [1/3], Step [320/4624], Loss: 3.3826, Perplexity: 29.4479\n",
      "Epoch [1/3], Step [340/4624], Loss: 3.2083, Perplexity: 24.7375\n",
      "Epoch [1/3], Step [360/4624], Loss: 3.7056, Perplexity: 40.6755\n",
      "Epoch [1/3], Step [380/4624], Loss: 3.3132, Perplexity: 27.4729\n",
      "Epoch [1/3], Step [400/4624], Loss: 3.1221, Perplexity: 22.6930\n",
      "Epoch [1/3], Step [420/4624], Loss: 3.2881, Perplexity: 26.7912\n",
      "Epoch [1/3], Step [440/4624], Loss: 3.2456, Perplexity: 25.6759\n",
      "Epoch [1/3], Step [460/4624], Loss: 3.0604, Perplexity: 21.3360\n",
      "Epoch [1/3], Step [480/4624], Loss: 3.0888, Perplexity: 21.9516\n",
      "Epoch [1/3], Step [500/4624], Loss: 3.5161, Perplexity: 33.6541\n",
      "Epoch [1/3], Step [520/4624], Loss: 2.9428, Perplexity: 18.9687\n",
      "Epoch [1/3], Step [540/4624], Loss: 2.9021, Perplexity: 18.2130\n",
      "Epoch [1/3], Step [560/4624], Loss: 3.0083, Perplexity: 20.2520\n",
      "Epoch [1/3], Step [580/4624], Loss: 3.6328, Perplexity: 37.8177\n",
      "Epoch [1/3], Step [600/4624], Loss: 2.9574, Perplexity: 19.2477\n",
      "Epoch [1/3], Step [620/4624], Loss: 2.9987, Perplexity: 20.0594\n",
      "Epoch [1/3], Step [640/4624], Loss: 3.1050, Perplexity: 22.3087\n",
      "Epoch [1/3], Step [660/4624], Loss: 3.3415, Perplexity: 28.2614\n",
      "Epoch [1/3], Step [680/4624], Loss: 2.9543, Perplexity: 19.1883\n",
      "Epoch [1/3], Step [700/4624], Loss: 3.2385, Perplexity: 25.4947\n",
      "Epoch [1/3], Step [720/4624], Loss: 2.9159, Perplexity: 18.4650\n",
      "Epoch [1/3], Step [740/4624], Loss: 3.1239, Perplexity: 22.7342\n",
      "Epoch [1/3], Step [760/4624], Loss: 2.8351, Perplexity: 17.0320\n",
      "Epoch [1/3], Step [780/4624], Loss: 2.9553, Perplexity: 19.2066\n",
      "Epoch [1/3], Step [800/4624], Loss: 2.9741, Perplexity: 19.5725\n",
      "Epoch [1/3], Step [820/4624], Loss: 2.8759, Perplexity: 17.7411\n",
      "Epoch [1/3], Step [840/4624], Loss: 2.6678, Perplexity: 14.4078\n",
      "Epoch [1/3], Step [860/4624], Loss: 2.7984, Perplexity: 16.4183\n",
      "Epoch [1/3], Step [880/4624], Loss: 2.8816, Perplexity: 17.8422\n",
      "Epoch [1/3], Step [900/4624], Loss: 2.6655, Perplexity: 14.3749\n",
      "Epoch [1/3], Step [920/4624], Loss: 2.7441, Perplexity: 15.5500\n",
      "Epoch [1/3], Step [940/4624], Loss: 4.0306, Perplexity: 56.2919\n",
      "Epoch [1/3], Step [960/4624], Loss: 2.6138, Perplexity: 13.6504\n",
      "Epoch [1/3], Step [980/4624], Loss: 2.7181, Perplexity: 15.1510\n",
      "Epoch [1/3], Step [1000/4624], Loss: 2.7631, Perplexity: 15.8486\n",
      "Epoch [1/3], Step [1020/4624], Loss: 2.9754, Perplexity: 19.5974\n",
      "Epoch [1/3], Step [1040/4624], Loss: 2.6471, Perplexity: 14.1133\n",
      "Epoch [1/3], Step [1060/4624], Loss: 2.7754, Perplexity: 16.0455\n",
      "Epoch [1/3], Step [1080/4624], Loss: 2.8189, Perplexity: 16.7581\n",
      "Epoch [1/3], Step [1100/4624], Loss: 2.6761, Perplexity: 14.5277\n",
      "Epoch [1/3], Step [1120/4624], Loss: 2.6793, Perplexity: 14.5750\n",
      "Epoch [1/3], Step [1140/4624], Loss: 2.5595, Perplexity: 12.9295\n",
      "Epoch [1/3], Step [1160/4624], Loss: 2.9114, Perplexity: 18.3817\n",
      "Epoch [1/3], Step [1180/4624], Loss: 2.5219, Perplexity: 12.4522\n",
      "Epoch [1/3], Step [1200/4624], Loss: 2.5838, Perplexity: 13.2468\n",
      "Epoch [1/3], Step [1220/4624], Loss: 3.0078, Perplexity: 20.2432\n",
      "Epoch [1/3], Step [1240/4624], Loss: 3.0967, Perplexity: 22.1245\n",
      "Epoch [1/3], Step [1260/4624], Loss: 2.4387, Perplexity: 11.4576\n",
      "Epoch [1/3], Step [1280/4624], Loss: 2.9798, Perplexity: 19.6838\n",
      "Epoch [1/3], Step [1300/4624], Loss: 2.6711, Perplexity: 14.4560\n",
      "Epoch [1/3], Step [1320/4624], Loss: 2.4519, Perplexity: 11.6109\n",
      "Epoch [1/3], Step [1340/4624], Loss: 2.5021, Perplexity: 12.2081\n",
      "Epoch [1/3], Step [1360/4624], Loss: 2.6036, Perplexity: 13.5120\n",
      "Epoch [1/3], Step [1380/4624], Loss: 2.4731, Perplexity: 11.8586\n",
      "Epoch [1/3], Step [1400/4624], Loss: 2.6090, Perplexity: 13.5861\n",
      "Epoch [1/3], Step [1420/4624], Loss: 2.4350, Perplexity: 11.4163\n",
      "Epoch [1/3], Step [1440/4624], Loss: 2.4660, Perplexity: 11.7755\n",
      "Epoch [1/3], Step [1460/4624], Loss: 2.8362, Perplexity: 17.0515\n",
      "Epoch [1/3], Step [1480/4624], Loss: 2.6413, Perplexity: 14.0319\n",
      "Epoch [1/3], Step [1500/4624], Loss: 2.3248, Perplexity: 10.2251\n",
      "Epoch [1/3], Step [1520/4624], Loss: 2.3579, Perplexity: 10.5686\n",
      "Epoch [1/3], Step [1540/4624], Loss: 3.4965, Perplexity: 32.9984\n",
      "Epoch [1/3], Step [1560/4624], Loss: 2.6163, Perplexity: 13.6854\n",
      "Epoch [1/3], Step [1580/4624], Loss: 2.9288, Perplexity: 18.7046\n",
      "Epoch [1/3], Step [1600/4624], Loss: 2.3992, Perplexity: 11.0141\n",
      "Epoch [1/3], Step [1620/4624], Loss: 2.5259, Perplexity: 12.5022\n",
      "Epoch [1/3], Step [1640/4624], Loss: 2.3808, Perplexity: 10.8133\n",
      "Epoch [1/3], Step [1660/4624], Loss: 2.6022, Perplexity: 13.4941\n",
      "Epoch [1/3], Step [1680/4624], Loss: 3.3910, Perplexity: 29.6942\n",
      "Epoch [1/3], Step [1700/4624], Loss: 2.3631, Perplexity: 10.6236\n",
      "Epoch [1/3], Step [1720/4624], Loss: 2.5405, Perplexity: 12.6857\n",
      "Epoch [1/3], Step [1740/4624], Loss: 2.3716, Perplexity: 10.7149\n",
      "Epoch [1/3], Step [1760/4624], Loss: 2.3076, Perplexity: 10.0503\n",
      "Epoch [1/3], Step [1780/4624], Loss: 2.5910, Perplexity: 13.3435\n",
      "Epoch [1/3], Step [1800/4624], Loss: 2.3449, Perplexity: 10.4326\n",
      "Epoch [1/3], Step [1820/4624], Loss: 2.3144, Perplexity: 10.1184\n",
      "Epoch [1/3], Step [1840/4624], Loss: 2.3899, Perplexity: 10.9125\n",
      "Epoch [1/3], Step [1860/4624], Loss: 2.5045, Perplexity: 12.2372\n",
      "Epoch [1/3], Step [1880/4624], Loss: 2.2356, Perplexity: 9.3517\n",
      "Epoch [1/3], Step [1900/4624], Loss: 2.3069, Perplexity: 10.0429\n",
      "Epoch [1/3], Step [1920/4624], Loss: 2.3592, Perplexity: 10.5821\n",
      "Epoch [1/3], Step [1940/4624], Loss: 2.3839, Perplexity: 10.8467\n",
      "Epoch [1/3], Step [1960/4624], Loss: 2.2996, Perplexity: 9.9699\n",
      "Epoch [1/3], Step [1980/4624], Loss: 2.4870, Perplexity: 12.0248\n",
      "Epoch [1/3], Step [2000/4624], Loss: 2.5562, Perplexity: 12.8870\n",
      "Epoch [1/3], Step [2020/4624], Loss: 2.3891, Perplexity: 10.9040\n",
      "Epoch [1/3], Step [2040/4624], Loss: 2.3147, Perplexity: 10.1218\n",
      "Epoch [1/3], Step [2060/4624], Loss: 2.3052, Perplexity: 10.0263\n",
      "Epoch [1/3], Step [2080/4624], Loss: 2.7749, Perplexity: 16.0376\n",
      "Epoch [1/3], Step [2100/4624], Loss: 2.3073, Perplexity: 10.0469\n",
      "Epoch [1/3], Step [2120/4624], Loss: 2.3196, Perplexity: 10.1716\n",
      "Epoch [1/3], Step [2140/4624], Loss: 2.2755, Perplexity: 9.7328\n",
      "Epoch [1/3], Step [2160/4624], Loss: 2.2792, Perplexity: 9.7686\n",
      "Epoch [1/3], Step [2180/4624], Loss: 2.3046, Perplexity: 10.0200\n",
      "Epoch [1/3], Step [2200/4624], Loss: 2.4649, Perplexity: 11.7619\n",
      "Epoch [1/3], Step [2220/4624], Loss: 2.5885, Perplexity: 13.3101\n",
      "Epoch [1/3], Step [2240/4624], Loss: 2.3499, Perplexity: 10.4848\n",
      "Epoch [1/3], Step [2260/4624], Loss: 2.2314, Perplexity: 9.3127\n",
      "Epoch [1/3], Step [2280/4624], Loss: 2.1763, Perplexity: 8.8134\n",
      "Epoch [1/3], Step [2300/4624], Loss: 2.3950, Perplexity: 10.9683\n",
      "Epoch [1/3], Step [2320/4624], Loss: 2.6288, Perplexity: 13.8566\n",
      "Epoch [1/3], Step [2340/4624], Loss: 3.5332, Perplexity: 34.2333\n",
      "Epoch [1/3], Step [2360/4624], Loss: 2.2682, Perplexity: 9.6616\n",
      "Epoch [1/3], Step [2380/4624], Loss: 2.2460, Perplexity: 9.4502\n",
      "Epoch [1/3], Step [2400/4624], Loss: 2.2932, Perplexity: 9.9070\n",
      "Epoch [1/3], Step [2420/4624], Loss: 2.3089, Perplexity: 10.0638\n",
      "Epoch [1/3], Step [2440/4624], Loss: 2.3605, Perplexity: 10.5960\n",
      "Epoch [1/3], Step [2460/4624], Loss: 2.4695, Perplexity: 11.8170\n",
      "Epoch [1/3], Step [2480/4624], Loss: 2.1920, Perplexity: 8.9535\n",
      "Epoch [1/3], Step [2500/4624], Loss: 2.2994, Perplexity: 9.9678\n",
      "Epoch [1/3], Step [2520/4624], Loss: 2.3544, Perplexity: 10.5313\n",
      "Epoch [1/3], Step [2540/4624], Loss: 2.2994, Perplexity: 9.9686\n",
      "Epoch [1/3], Step [2560/4624], Loss: 2.1873, Perplexity: 8.9111\n",
      "Epoch [1/3], Step [2580/4624], Loss: 2.3250, Perplexity: 10.2264\n",
      "Epoch [1/3], Step [2600/4624], Loss: 2.1515, Perplexity: 8.5981\n",
      "Epoch [1/3], Step [2620/4624], Loss: 2.3708, Perplexity: 10.7063\n",
      "Epoch [1/3], Step [2640/4624], Loss: 2.1357, Perplexity: 8.4627\n",
      "Epoch [1/3], Step [2660/4624], Loss: 2.3060, Perplexity: 10.0341\n",
      "Epoch [1/3], Step [2680/4624], Loss: 2.3007, Perplexity: 9.9808\n",
      "Epoch [1/3], Step [2700/4624], Loss: 2.1577, Perplexity: 8.6508\n",
      "Epoch [1/3], Step [2720/4624], Loss: 2.5813, Perplexity: 13.2140\n",
      "Epoch [1/3], Step [2740/4624], Loss: 2.0818, Perplexity: 8.0189\n",
      "Epoch [1/3], Step [2760/4624], Loss: 2.2941, Perplexity: 9.9155\n",
      "Epoch [1/3], Step [2780/4624], Loss: 2.2864, Perplexity: 9.8397\n",
      "Epoch [1/3], Step [2800/4624], Loss: 2.0776, Perplexity: 7.9851\n",
      "Epoch [1/3], Step [2820/4624], Loss: 2.5732, Perplexity: 13.1072\n",
      "Epoch [1/3], Step [2840/4624], Loss: 2.2465, Perplexity: 9.4548\n",
      "Epoch [1/3], Step [2860/4624], Loss: 2.1544, Perplexity: 8.6231\n",
      "Epoch [1/3], Step [2880/4624], Loss: 2.3040, Perplexity: 10.0138\n",
      "Epoch [1/3], Step [2900/4624], Loss: 2.1988, Perplexity: 9.0140\n",
      "Epoch [1/3], Step [2920/4624], Loss: 2.5462, Perplexity: 12.7585\n",
      "Epoch [1/3], Step [2940/4624], Loss: 2.4820, Perplexity: 11.9654\n",
      "Epoch [1/3], Step [2960/4624], Loss: 2.3472, Perplexity: 10.4557\n",
      "Epoch [1/3], Step [2980/4624], Loss: 2.2948, Perplexity: 9.9220\n",
      "Epoch [1/3], Step [3000/4624], Loss: 2.2421, Perplexity: 9.4132\n",
      "Epoch [1/3], Step [3020/4624], Loss: 2.1629, Perplexity: 8.6964\n",
      "Epoch [1/3], Step [3040/4624], Loss: 2.3354, Perplexity: 10.3332\n",
      "Epoch [1/3], Step [3060/4624], Loss: 2.2603, Perplexity: 9.5864\n",
      "Epoch [1/3], Step [3080/4624], Loss: 2.2233, Perplexity: 9.2374\n",
      "Epoch [1/3], Step [3100/4624], Loss: 2.3086, Perplexity: 10.0601\n",
      "Epoch [1/3], Step [3120/4624], Loss: 2.4000, Perplexity: 11.0231\n",
      "Epoch [1/3], Step [3140/4624], Loss: 2.1775, Perplexity: 8.8246\n",
      "Epoch [1/3], Step [3160/4624], Loss: 2.1173, Perplexity: 8.3083\n",
      "Epoch [1/3], Step [3180/4624], Loss: 2.1626, Perplexity: 8.6941\n",
      "Epoch [1/3], Step [3200/4624], Loss: 2.2657, Perplexity: 9.6380\n",
      "Epoch [1/3], Step [3220/4624], Loss: 2.1173, Perplexity: 8.3089\n",
      "Epoch [1/3], Step [3240/4624], Loss: 2.3291, Perplexity: 10.2691\n",
      "Epoch [1/3], Step [3260/4624], Loss: 2.0617, Perplexity: 7.8597\n",
      "Epoch [1/3], Step [3280/4624], Loss: 2.2043, Perplexity: 9.0635\n",
      "Epoch [1/3], Step [3300/4624], Loss: 2.1136, Perplexity: 8.2779\n",
      "Epoch [1/3], Step [3320/4624], Loss: 2.2436, Perplexity: 9.4271\n",
      "Epoch [1/3], Step [3340/4624], Loss: 2.2125, Perplexity: 9.1387\n",
      "Epoch [1/3], Step [3360/4624], Loss: 2.1976, Perplexity: 9.0030\n",
      "Epoch [1/3], Step [3380/4624], Loss: 2.1966, Perplexity: 8.9947\n",
      "Epoch [1/3], Step [3400/4624], Loss: 2.1567, Perplexity: 8.6422\n",
      "Epoch [1/3], Step [3420/4624], Loss: 2.1521, Perplexity: 8.6025\n",
      "Epoch [1/3], Step [3440/4624], Loss: 2.1064, Perplexity: 8.2185\n",
      "Epoch [1/3], Step [3460/4624], Loss: 2.3554, Perplexity: 10.5426\n",
      "Epoch [1/3], Step [3480/4624], Loss: 2.2956, Perplexity: 9.9303\n",
      "Epoch [1/3], Step [3500/4624], Loss: 2.0308, Perplexity: 7.6203\n",
      "Epoch [1/3], Step [3520/4624], Loss: 2.2142, Perplexity: 9.1540\n",
      "Epoch [1/3], Step [3540/4624], Loss: 2.3079, Perplexity: 10.0533\n",
      "Epoch [1/3], Step [3560/4624], Loss: 2.2933, Perplexity: 9.9072\n",
      "Epoch [1/3], Step [3580/4624], Loss: 2.5684, Perplexity: 13.0448\n",
      "Epoch [1/3], Step [3600/4624], Loss: 2.0854, Perplexity: 8.0478\n",
      "Epoch [1/3], Step [3620/4624], Loss: 2.1617, Perplexity: 8.6861\n",
      "Epoch [1/3], Step [3640/4624], Loss: 2.3158, Perplexity: 10.1335\n",
      "Epoch [1/3], Step [3660/4624], Loss: 2.1677, Perplexity: 8.7378\n",
      "Epoch [1/3], Step [3680/4624], Loss: 2.2944, Perplexity: 9.9182\n",
      "Epoch [1/3], Step [3700/4624], Loss: 2.0859, Perplexity: 8.0516\n",
      "Epoch [1/3], Step [3720/4624], Loss: 2.1740, Perplexity: 8.7930\n",
      "Epoch [1/3], Step [3740/4624], Loss: 2.0868, Perplexity: 8.0588\n",
      "Epoch [1/3], Step [3760/4624], Loss: 2.0956, Perplexity: 8.1304\n",
      "Epoch [1/3], Step [3780/4624], Loss: 2.2099, Perplexity: 9.1153\n",
      "Epoch [1/3], Step [3800/4624], Loss: 2.2180, Perplexity: 9.1888\n",
      "Epoch [1/3], Step [3820/4624], Loss: 2.1519, Perplexity: 8.6015\n",
      "Epoch [1/3], Step [3840/4624], Loss: 2.5717, Perplexity: 13.0886\n",
      "Epoch [1/3], Step [3860/4624], Loss: 2.0715, Perplexity: 7.9365\n",
      "Epoch [1/3], Step [3880/4624], Loss: 2.1542, Perplexity: 8.6212\n",
      "Epoch [1/3], Step [3900/4624], Loss: 2.0398, Perplexity: 7.6893\n",
      "Epoch [1/3], Step [3920/4624], Loss: 2.0845, Perplexity: 8.0407\n",
      "Epoch [1/3], Step [3940/4624], Loss: 2.0534, Perplexity: 7.7942\n",
      "Epoch [1/3], Step [3960/4624], Loss: 2.1156, Perplexity: 8.2947\n",
      "Epoch [1/3], Step [3980/4624], Loss: 2.7927, Perplexity: 16.3245\n",
      "Epoch [1/3], Step [4000/4624], Loss: 2.1010, Perplexity: 8.1745\n",
      "Epoch [1/3], Step [4020/4624], Loss: 2.2450, Perplexity: 9.4403\n",
      "Epoch [1/3], Step [4040/4624], Loss: 2.2758, Perplexity: 9.7358\n",
      "Epoch [1/3], Step [4060/4624], Loss: 2.2377, Perplexity: 9.3716\n",
      "Epoch [1/3], Step [4080/4624], Loss: 2.0537, Perplexity: 7.7967\n",
      "Epoch [1/3], Step [4100/4624], Loss: 2.5962, Perplexity: 13.4128\n",
      "Epoch [1/3], Step [4120/4624], Loss: 2.0184, Perplexity: 7.5263\n",
      "Epoch [1/3], Step [4140/4624], Loss: 2.1124, Perplexity: 8.2678\n",
      "Epoch [1/3], Step [4160/4624], Loss: 2.1229, Perplexity: 8.3553\n",
      "Epoch [1/3], Step [4180/4624], Loss: 2.1506, Perplexity: 8.5902\n",
      "Epoch [1/3], Step [4200/4624], Loss: 2.1714, Perplexity: 8.7701\n",
      "Epoch [1/3], Step [4220/4624], Loss: 2.4089, Perplexity: 11.1222\n",
      "Epoch [1/3], Step [4240/4624], Loss: 2.0379, Perplexity: 7.6746\n",
      "Epoch [1/3], Step [4260/4624], Loss: 2.1376, Perplexity: 8.4790\n",
      "Epoch [1/3], Step [4280/4624], Loss: 2.3184, Perplexity: 10.1593\n",
      "Epoch [1/3], Step [4300/4624], Loss: 2.0476, Perplexity: 7.7495\n",
      "Epoch [1/3], Step [4320/4624], Loss: 2.0439, Perplexity: 7.7208\n",
      "Epoch [1/3], Step [4340/4624], Loss: 2.0151, Perplexity: 7.5012\n",
      "Epoch [1/3], Step [4360/4624], Loss: 2.3063, Perplexity: 10.0369\n",
      "Epoch [1/3], Step [4380/4624], Loss: 2.3385, Perplexity: 10.3656\n",
      "Epoch [1/3], Step [4400/4624], Loss: 2.1871, Perplexity: 8.9096\n",
      "Epoch [1/3], Step [4420/4624], Loss: 2.1719, Perplexity: 8.7751\n",
      "Epoch [1/3], Step [4440/4624], Loss: 2.1506, Perplexity: 8.5897\n",
      "Epoch [1/3], Step [4460/4624], Loss: 2.2151, Perplexity: 9.1619\n",
      "Epoch [1/3], Step [4480/4624], Loss: 1.9185, Perplexity: 6.8106\n",
      "Epoch [1/3], Step [4500/4624], Loss: 2.1569, Perplexity: 8.6445\n",
      "Epoch [1/3], Step [4520/4624], Loss: 2.2774, Perplexity: 9.7509\n",
      "Epoch [1/3], Step [4540/4624], Loss: 2.0763, Perplexity: 7.9748\n",
      "Epoch [1/3], Step [4560/4624], Loss: 3.5301, Perplexity: 34.1287\n",
      "Epoch [1/3], Step [4580/4624], Loss: 2.4946, Perplexity: 12.1173\n",
      "Epoch [1/3], Step [4600/4624], Loss: 2.1213, Perplexity: 8.3420\n",
      "Epoch [1/3], Step [4620/4624], Loss: 2.1674, Perplexity: 8.7359\n",
      "Epoch [2/3], Step [20/4624], Loss: 2.0417, Perplexity: 7.7035\n",
      "Epoch [2/3], Step [40/4624], Loss: 2.0777, Perplexity: 7.9865\n",
      "Epoch [2/3], Step [60/4624], Loss: 1.9929, Perplexity: 7.3369\n",
      "Epoch [2/3], Step [80/4624], Loss: 2.1658, Perplexity: 8.7218\n",
      "Epoch [2/3], Step [100/4624], Loss: 2.0230, Perplexity: 7.5608\n",
      "Epoch [2/3], Step [120/4624], Loss: 2.2877, Perplexity: 9.8527\n",
      "Epoch [2/3], Step [140/4624], Loss: 2.2061, Perplexity: 9.0803\n",
      "Epoch [2/3], Step [160/4624], Loss: 2.6667, Perplexity: 14.3930\n",
      "Epoch [2/3], Step [180/4624], Loss: 2.1502, Perplexity: 8.5869\n",
      "Epoch [2/3], Step [200/4624], Loss: 2.0398, Perplexity: 7.6887\n",
      "Epoch [2/3], Step [220/4624], Loss: 2.0300, Perplexity: 7.6142\n",
      "Epoch [2/3], Step [240/4624], Loss: 2.0813, Perplexity: 8.0149\n",
      "Epoch [2/3], Step [260/4624], Loss: 2.0861, Perplexity: 8.0531\n",
      "Epoch [2/3], Step [280/4624], Loss: 1.9708, Perplexity: 7.1761\n",
      "Epoch [2/3], Step [300/4624], Loss: 2.0610, Perplexity: 7.8539\n",
      "Epoch [2/3], Step [320/4624], Loss: 2.1990, Perplexity: 9.0156\n",
      "Epoch [2/3], Step [340/4624], Loss: 2.1469, Perplexity: 8.5586\n",
      "Epoch [2/3], Step [360/4624], Loss: 1.9870, Perplexity: 7.2938\n",
      "Epoch [2/3], Step [380/4624], Loss: 2.1975, Perplexity: 9.0025\n",
      "Epoch [2/3], Step [400/4624], Loss: 2.0908, Perplexity: 8.0910\n",
      "Epoch [2/3], Step [420/4624], Loss: 2.2766, Perplexity: 9.7439\n",
      "Epoch [2/3], Step [440/4624], Loss: 2.8287, Perplexity: 16.9239\n",
      "Epoch [2/3], Step [460/4624], Loss: 2.3394, Perplexity: 10.3750\n",
      "Epoch [2/3], Step [480/4624], Loss: 2.1572, Perplexity: 8.6466\n",
      "Epoch [2/3], Step [500/4624], Loss: 2.1488, Perplexity: 8.5746\n",
      "Epoch [2/3], Step [520/4624], Loss: 2.2731, Perplexity: 9.7095\n",
      "Epoch [2/3], Step [540/4624], Loss: 2.0201, Perplexity: 7.5388\n",
      "Epoch [2/3], Step [560/4624], Loss: 2.0277, Perplexity: 7.5963\n",
      "Epoch [2/3], Step [580/4624], Loss: 2.1475, Perplexity: 8.5631\n",
      "Epoch [2/3], Step [600/4624], Loss: 2.1835, Perplexity: 8.8777\n",
      "Epoch [2/3], Step [620/4624], Loss: 2.2331, Perplexity: 9.3284\n",
      "Epoch [2/3], Step [640/4624], Loss: 2.3007, Perplexity: 9.9808\n",
      "Epoch [2/3], Step [660/4624], Loss: 2.0291, Perplexity: 7.6072\n",
      "Epoch [2/3], Step [680/4624], Loss: 2.1898, Perplexity: 8.9337\n",
      "Epoch [2/3], Step [700/4624], Loss: 2.0113, Perplexity: 7.4731\n",
      "Epoch [2/3], Step [720/4624], Loss: 2.1306, Perplexity: 8.4199\n",
      "Epoch [2/3], Step [740/4624], Loss: 2.1092, Perplexity: 8.2414\n",
      "Epoch [2/3], Step [760/4624], Loss: 2.0002, Perplexity: 7.3904\n",
      "Epoch [2/3], Step [780/4624], Loss: 2.4521, Perplexity: 11.6128\n",
      "Epoch [2/3], Step [800/4624], Loss: 2.0761, Perplexity: 7.9731\n",
      "Epoch [2/3], Step [820/4624], Loss: 2.2304, Perplexity: 9.3040\n",
      "Epoch [2/3], Step [840/4624], Loss: 1.9934, Perplexity: 7.3403\n",
      "Epoch [2/3], Step [860/4624], Loss: 2.0007, Perplexity: 7.3942\n",
      "Epoch [2/3], Step [880/4624], Loss: 2.1637, Perplexity: 8.7036\n",
      "Epoch [2/3], Step [900/4624], Loss: 2.0390, Perplexity: 7.6829\n",
      "Epoch [2/3], Step [920/4624], Loss: 2.1487, Perplexity: 8.5741\n",
      "Epoch [2/3], Step [940/4624], Loss: 2.0661, Perplexity: 7.8936\n",
      "Epoch [2/3], Step [960/4624], Loss: 2.4866, Perplexity: 12.0206\n",
      "Epoch [2/3], Step [980/4624], Loss: 2.3877, Perplexity: 10.8885\n",
      "Epoch [2/3], Step [1000/4624], Loss: 2.1263, Perplexity: 8.3835\n",
      "Epoch [2/3], Step [1020/4624], Loss: 2.3243, Perplexity: 10.2193\n",
      "Epoch [2/3], Step [1040/4624], Loss: 2.0111, Perplexity: 7.4714\n",
      "Epoch [2/3], Step [1060/4624], Loss: 2.0046, Perplexity: 7.4232\n",
      "Epoch [2/3], Step [1080/4624], Loss: 2.1082, Perplexity: 8.2338\n",
      "Epoch [2/3], Step [1100/4624], Loss: 2.3611, Perplexity: 10.6030\n",
      "Epoch [2/3], Step [1120/4624], Loss: 1.9911, Perplexity: 7.3234\n",
      "Epoch [2/3], Step [1140/4624], Loss: 2.3530, Perplexity: 10.5168\n",
      "Epoch [2/3], Step [1160/4624], Loss: 2.2247, Perplexity: 9.2506\n",
      "Epoch [2/3], Step [1180/4624], Loss: 1.9452, Perplexity: 6.9948\n",
      "Epoch [2/3], Step [1200/4624], Loss: 3.4010, Perplexity: 29.9949\n",
      "Epoch [2/3], Step [1220/4624], Loss: 2.2468, Perplexity: 9.4579\n",
      "Epoch [2/3], Step [1240/4624], Loss: 2.0495, Perplexity: 7.7643\n",
      "Epoch [2/3], Step [1260/4624], Loss: 2.2608, Perplexity: 9.5906\n",
      "Epoch [2/3], Step [1280/4624], Loss: 1.9030, Perplexity: 6.7062\n",
      "Epoch [2/3], Step [1300/4624], Loss: 1.9281, Perplexity: 6.8761\n",
      "Epoch [2/3], Step [1320/4624], Loss: 1.9857, Perplexity: 7.2845\n",
      "Epoch [2/3], Step [1340/4624], Loss: 2.1753, Perplexity: 8.8050\n",
      "Epoch [2/3], Step [1360/4624], Loss: 2.0428, Perplexity: 7.7124\n",
      "Epoch [2/3], Step [1380/4624], Loss: 1.9838, Perplexity: 7.2701\n",
      "Epoch [2/3], Step [1400/4624], Loss: 2.0728, Perplexity: 7.9470\n",
      "Epoch [2/3], Step [1420/4624], Loss: 2.0607, Perplexity: 7.8514\n",
      "Epoch [2/3], Step [1440/4624], Loss: 2.0602, Perplexity: 7.8478\n",
      "Epoch [2/3], Step [1460/4624], Loss: 2.2683, Perplexity: 9.6630\n",
      "Epoch [2/3], Step [1480/4624], Loss: 2.3295, Perplexity: 10.2723\n",
      "Epoch [2/3], Step [1500/4624], Loss: 2.0409, Perplexity: 7.6975\n",
      "Epoch [2/3], Step [1520/4624], Loss: 2.1442, Perplexity: 8.5355\n",
      "Epoch [2/3], Step [1540/4624], Loss: 2.4920, Perplexity: 12.0859\n",
      "Epoch [2/3], Step [1560/4624], Loss: 2.0109, Perplexity: 7.4700\n",
      "Epoch [2/3], Step [1580/4624], Loss: 2.1363, Perplexity: 8.4680\n",
      "Epoch [2/3], Step [1600/4624], Loss: 2.0624, Perplexity: 7.8648\n",
      "Epoch [2/3], Step [1620/4624], Loss: 2.1426, Perplexity: 8.5217\n",
      "Epoch [2/3], Step [1640/4624], Loss: 2.0042, Perplexity: 7.4204\n",
      "Epoch [2/3], Step [1660/4624], Loss: 1.9115, Perplexity: 6.7631\n",
      "Epoch [2/3], Step [1680/4624], Loss: 2.0807, Perplexity: 8.0099\n",
      "Epoch [2/3], Step [1700/4624], Loss: 1.8911, Perplexity: 6.6268\n",
      "Epoch [2/3], Step [1720/4624], Loss: 2.2102, Perplexity: 9.1175\n",
      "Epoch [2/3], Step [1740/4624], Loss: 2.5033, Perplexity: 12.2224\n",
      "Epoch [2/3], Step [1760/4624], Loss: 1.9196, Perplexity: 6.8180\n",
      "Epoch [2/3], Step [1780/4624], Loss: 1.8129, Perplexity: 6.1283\n",
      "Epoch [2/3], Step [1800/4624], Loss: 2.0346, Perplexity: 7.6493\n",
      "Epoch [2/3], Step [1820/4624], Loss: 2.1150, Perplexity: 8.2894\n",
      "Epoch [2/3], Step [1840/4624], Loss: 2.0038, Perplexity: 7.4171\n",
      "Epoch [2/3], Step [1860/4624], Loss: 1.9286, Perplexity: 6.8799\n",
      "Epoch [2/3], Step [1880/4624], Loss: 2.1268, Perplexity: 8.3879\n",
      "Epoch [2/3], Step [1900/4624], Loss: 1.9758, Perplexity: 7.2126\n",
      "Epoch [2/3], Step [1920/4624], Loss: 2.0565, Perplexity: 7.8186\n",
      "Epoch [2/3], Step [1940/4624], Loss: 2.0847, Perplexity: 8.0424\n",
      "Epoch [2/3], Step [1960/4624], Loss: 2.2545, Perplexity: 9.5307\n",
      "Epoch [2/3], Step [1980/4624], Loss: 2.2282, Perplexity: 9.2833\n",
      "Epoch [2/3], Step [2000/4624], Loss: 2.0331, Perplexity: 7.6377\n",
      "Epoch [2/3], Step [2020/4624], Loss: 2.0669, Perplexity: 7.9001\n",
      "Epoch [2/3], Step [2040/4624], Loss: 2.0574, Perplexity: 7.8256\n",
      "Epoch [2/3], Step [2060/4624], Loss: 2.0594, Perplexity: 7.8413\n",
      "Epoch [2/3], Step [2080/4624], Loss: 2.3981, Perplexity: 11.0028\n",
      "Epoch [2/3], Step [2100/4624], Loss: 2.1441, Perplexity: 8.5339\n",
      "Epoch [2/3], Step [2120/4624], Loss: 2.0819, Perplexity: 8.0200\n",
      "Epoch [2/3], Step [2140/4624], Loss: 2.0996, Perplexity: 8.1627\n",
      "Epoch [2/3], Step [2160/4624], Loss: 2.1825, Perplexity: 8.8686\n",
      "Epoch [2/3], Step [2180/4624], Loss: 2.0864, Perplexity: 8.0559\n",
      "Epoch [2/3], Step [2200/4624], Loss: 1.9427, Perplexity: 6.9774\n",
      "Epoch [2/3], Step [2220/4624], Loss: 2.0808, Perplexity: 8.0113\n",
      "Epoch [2/3], Step [2240/4624], Loss: 2.4235, Perplexity: 11.2850\n",
      "Epoch [2/3], Step [2260/4624], Loss: 2.0460, Perplexity: 7.7373\n",
      "Epoch [2/3], Step [2280/4624], Loss: 1.9777, Perplexity: 7.2260\n",
      "Epoch [2/3], Step [2300/4624], Loss: 2.0910, Perplexity: 8.0927\n",
      "Epoch [2/3], Step [2320/4624], Loss: 2.1796, Perplexity: 8.8430\n",
      "Epoch [2/3], Step [2340/4624], Loss: 2.0700, Perplexity: 7.9249\n",
      "Epoch [2/3], Step [2360/4624], Loss: 2.1658, Perplexity: 8.7212\n",
      "Epoch [2/3], Step [2380/4624], Loss: 2.7322, Perplexity: 15.3669\n",
      "Epoch [2/3], Step [2400/4624], Loss: 2.1021, Perplexity: 8.1832\n",
      "Epoch [2/3], Step [2420/4624], Loss: 2.1632, Perplexity: 8.6990\n",
      "Epoch [2/3], Step [2440/4624], Loss: 1.9731, Perplexity: 7.1932\n",
      "Epoch [2/3], Step [2460/4624], Loss: 2.0761, Perplexity: 7.9733\n",
      "Epoch [2/3], Step [2480/4624], Loss: 2.0827, Perplexity: 8.0265\n",
      "Epoch [2/3], Step [2500/4624], Loss: 2.0052, Perplexity: 7.4277\n",
      "Epoch [2/3], Step [2520/4624], Loss: 2.0446, Perplexity: 7.7257\n",
      "Epoch [2/3], Step [2540/4624], Loss: 1.9847, Perplexity: 7.2768\n",
      "Epoch [2/3], Step [2560/4624], Loss: 2.0332, Perplexity: 7.6381\n",
      "Epoch [2/3], Step [2580/4624], Loss: 2.3549, Perplexity: 10.5375\n",
      "Epoch [2/3], Step [2600/4624], Loss: 2.0990, Perplexity: 8.1577\n",
      "Epoch [2/3], Step [2620/4624], Loss: 1.9995, Perplexity: 7.3855\n",
      "Epoch [2/3], Step [2640/4624], Loss: 2.3332, Perplexity: 10.3113\n",
      "Epoch [2/3], Step [2660/4624], Loss: 2.1644, Perplexity: 8.7089\n",
      "Epoch [2/3], Step [2680/4624], Loss: 2.1139, Perplexity: 8.2804\n",
      "Epoch [2/3], Step [2700/4624], Loss: 2.0978, Perplexity: 8.1483\n",
      "Epoch [2/3], Step [2720/4624], Loss: 1.9481, Perplexity: 7.0152\n",
      "Epoch [2/3], Step [2740/4624], Loss: 2.0106, Perplexity: 7.4674\n",
      "Epoch [2/3], Step [2760/4624], Loss: 2.2990, Perplexity: 9.9645\n",
      "Epoch [2/3], Step [2780/4624], Loss: 2.3127, Perplexity: 10.1017\n",
      "Epoch [2/3], Step [2800/4624], Loss: 2.0956, Perplexity: 8.1299\n",
      "Epoch [2/3], Step [2820/4624], Loss: 1.9522, Perplexity: 7.0438\n",
      "Epoch [2/3], Step [2840/4624], Loss: 1.8659, Perplexity: 6.4617\n",
      "Epoch [2/3], Step [2860/4624], Loss: 1.8935, Perplexity: 6.6429\n",
      "Epoch [2/3], Step [2880/4624], Loss: 2.0031, Perplexity: 7.4121\n",
      "Epoch [2/3], Step [2900/4624], Loss: 2.1132, Perplexity: 8.2749\n",
      "Epoch [2/3], Step [2920/4624], Loss: 2.2232, Perplexity: 9.2369\n",
      "Epoch [2/3], Step [2940/4624], Loss: 1.9067, Perplexity: 6.7310\n",
      "Epoch [2/3], Step [2960/4624], Loss: 2.0792, Perplexity: 7.9984\n",
      "Epoch [2/3], Step [2980/4624], Loss: 2.0412, Perplexity: 7.7002\n",
      "Epoch [2/3], Step [3000/4624], Loss: 1.9463, Perplexity: 7.0027\n",
      "Epoch [2/3], Step [3020/4624], Loss: 2.0831, Perplexity: 8.0295\n",
      "Epoch [2/3], Step [3040/4624], Loss: 1.9626, Perplexity: 7.1180\n",
      "Epoch [2/3], Step [3060/4624], Loss: 1.9264, Perplexity: 6.8650\n",
      "Epoch [2/3], Step [3080/4624], Loss: 1.9737, Perplexity: 7.1976\n",
      "Epoch [2/3], Step [3100/4624], Loss: 2.0156, Perplexity: 7.5055\n",
      "Epoch [2/3], Step [3120/4624], Loss: 1.9606, Perplexity: 7.1034\n",
      "Epoch [2/3], Step [3140/4624], Loss: 1.9509, Perplexity: 7.0352\n",
      "Epoch [2/3], Step [3160/4624], Loss: 2.1387, Perplexity: 8.4888\n",
      "Epoch [2/3], Step [3180/4624], Loss: 2.0614, Perplexity: 7.8571\n",
      "Epoch [2/3], Step [3200/4624], Loss: 1.8675, Perplexity: 6.4721\n",
      "Epoch [2/3], Step [3220/4624], Loss: 1.9820, Perplexity: 7.2569\n",
      "Epoch [2/3], Step [3240/4624], Loss: 2.2602, Perplexity: 9.5848\n",
      "Epoch [2/3], Step [3260/4624], Loss: 1.8918, Perplexity: 6.6313\n",
      "Epoch [2/3], Step [3280/4624], Loss: 2.2352, Perplexity: 9.3484\n",
      "Epoch [2/3], Step [3300/4624], Loss: 2.0205, Perplexity: 7.5423\n",
      "Epoch [2/3], Step [3320/4624], Loss: 1.8234, Perplexity: 6.1929\n",
      "Epoch [2/3], Step [3340/4624], Loss: 2.0061, Perplexity: 7.4339\n",
      "Epoch [2/3], Step [3360/4624], Loss: 2.0925, Perplexity: 8.1054\n",
      "Epoch [2/3], Step [3380/4624], Loss: 2.3976, Perplexity: 10.9966\n",
      "Epoch [2/3], Step [3400/4624], Loss: 1.9020, Perplexity: 6.6991\n",
      "Epoch [2/3], Step [3420/4624], Loss: 2.2859, Perplexity: 9.8350\n",
      "Epoch [2/3], Step [3440/4624], Loss: 1.9510, Perplexity: 7.0358\n",
      "Epoch [2/3], Step [3460/4624], Loss: 2.0604, Perplexity: 7.8490\n",
      "Epoch [2/3], Step [3480/4624], Loss: 1.9814, Perplexity: 7.2526\n",
      "Epoch [2/3], Step [3500/4624], Loss: 2.0467, Perplexity: 7.7421\n",
      "Epoch [2/3], Step [3520/4624], Loss: 2.0010, Perplexity: 7.3964\n",
      "Epoch [2/3], Step [3540/4624], Loss: 1.9670, Perplexity: 7.1492\n",
      "Epoch [2/3], Step [3560/4624], Loss: 1.9886, Perplexity: 7.3057\n",
      "Epoch [2/3], Step [3580/4624], Loss: 2.4450, Perplexity: 11.5309\n",
      "Epoch [2/3], Step [3600/4624], Loss: 2.8213, Perplexity: 16.7995\n",
      "Epoch [2/3], Step [3620/4624], Loss: 2.0292, Perplexity: 7.6082\n",
      "Epoch [2/3], Step [3640/4624], Loss: 1.9052, Perplexity: 6.7205\n",
      "Epoch [2/3], Step [3660/4624], Loss: 2.0498, Perplexity: 7.7661\n",
      "Epoch [2/3], Step [3680/4624], Loss: 1.9964, Perplexity: 7.3627\n",
      "Epoch [2/3], Step [3700/4624], Loss: 1.9228, Perplexity: 6.8403\n",
      "Epoch [2/3], Step [3720/4624], Loss: 1.9583, Perplexity: 7.0871\n",
      "Epoch [2/3], Step [3740/4624], Loss: 1.9187, Perplexity: 6.8121\n",
      "Epoch [2/3], Step [3760/4624], Loss: 1.9367, Perplexity: 6.9361\n",
      "Epoch [2/3], Step [3780/4624], Loss: 1.9524, Perplexity: 7.0455\n",
      "Epoch [2/3], Step [3800/4624], Loss: 2.3647, Perplexity: 10.6413\n",
      "Epoch [2/3], Step [3820/4624], Loss: 1.9906, Perplexity: 7.3201\n",
      "Epoch [2/3], Step [3840/4624], Loss: 2.0498, Perplexity: 7.7666\n",
      "Epoch [2/3], Step [3860/4624], Loss: 1.8702, Perplexity: 6.4897\n",
      "Epoch [2/3], Step [3880/4624], Loss: 1.9287, Perplexity: 6.8806\n",
      "Epoch [2/3], Step [3900/4624], Loss: 2.1742, Perplexity: 8.7953\n",
      "Epoch [2/3], Step [3920/4624], Loss: 1.9322, Perplexity: 6.9045\n",
      "Epoch [2/3], Step [3940/4624], Loss: 1.8812, Perplexity: 6.5616\n",
      "Epoch [2/3], Step [3960/4624], Loss: 1.9621, Perplexity: 7.1143\n",
      "Epoch [2/3], Step [3980/4624], Loss: 2.0271, Perplexity: 7.5923\n",
      "Epoch [2/3], Step [4000/4624], Loss: 1.8186, Perplexity: 6.1630\n",
      "Epoch [2/3], Step [4020/4624], Loss: 2.4317, Perplexity: 11.3780\n",
      "Epoch [2/3], Step [4040/4624], Loss: 1.9634, Perplexity: 7.1237\n",
      "Epoch [2/3], Step [4060/4624], Loss: 1.9494, Perplexity: 7.0242\n",
      "Epoch [2/3], Step [4080/4624], Loss: 2.4740, Perplexity: 11.8695\n",
      "Epoch [2/3], Step [4100/4624], Loss: 2.0542, Perplexity: 7.8009\n",
      "Epoch [2/3], Step [4120/4624], Loss: 1.8335, Perplexity: 6.2558\n",
      "Epoch [2/3], Step [4140/4624], Loss: 1.9095, Perplexity: 6.7499\n",
      "Epoch [2/3], Step [4160/4624], Loss: 1.8724, Perplexity: 6.5038\n",
      "Epoch [2/3], Step [4180/4624], Loss: 2.0316, Perplexity: 7.6261\n",
      "Epoch [2/3], Step [4200/4624], Loss: 2.0574, Perplexity: 7.8258\n",
      "Epoch [2/3], Step [4220/4624], Loss: 1.9154, Perplexity: 6.7894\n",
      "Epoch [2/3], Step [4240/4624], Loss: 2.0245, Perplexity: 7.5727\n",
      "Epoch [2/3], Step [4260/4624], Loss: 2.0935, Perplexity: 8.1133\n",
      "Epoch [2/3], Step [4280/4624], Loss: 2.7310, Perplexity: 15.3486\n",
      "Epoch [2/3], Step [4300/4624], Loss: 1.9354, Perplexity: 6.9265\n",
      "Epoch [2/3], Step [4320/4624], Loss: 1.8743, Perplexity: 6.5165\n",
      "Epoch [2/3], Step [4340/4624], Loss: 2.1596, Perplexity: 8.6675\n",
      "Epoch [2/3], Step [4360/4624], Loss: 1.9265, Perplexity: 6.8656\n",
      "Epoch [2/3], Step [4380/4624], Loss: 2.5364, Perplexity: 12.6345\n",
      "Epoch [2/3], Step [4400/4624], Loss: 1.9434, Perplexity: 6.9826\n",
      "Epoch [2/3], Step [4420/4624], Loss: 2.0861, Perplexity: 8.0532\n",
      "Epoch [2/3], Step [4440/4624], Loss: 1.8695, Perplexity: 6.4852\n",
      "Epoch [2/3], Step [4460/4624], Loss: 2.0093, Perplexity: 7.4580\n",
      "Epoch [2/3], Step [4480/4624], Loss: 2.0381, Perplexity: 7.6764\n",
      "Epoch [2/3], Step [4500/4624], Loss: 2.0366, Perplexity: 7.6644\n",
      "Epoch [2/3], Step [4520/4624], Loss: 2.2083, Perplexity: 9.1005\n",
      "Epoch [2/3], Step [4540/4624], Loss: 1.9763, Perplexity: 7.2160\n",
      "Epoch [2/3], Step [4560/4624], Loss: 1.9707, Perplexity: 7.1760\n",
      "Epoch [2/3], Step [4580/4624], Loss: 2.1245, Perplexity: 8.3690\n",
      "Epoch [2/3], Step [4600/4624], Loss: 1.9599, Perplexity: 7.0986\n",
      "Epoch [2/3], Step [4620/4624], Loss: 1.9968, Perplexity: 7.3658\n",
      "Epoch [3/3], Step [20/4624], Loss: 2.0867, Perplexity: 8.0581\n",
      "Epoch [3/3], Step [40/4624], Loss: 1.9797, Perplexity: 7.2407\n",
      "Epoch [3/3], Step [60/4624], Loss: 2.0069, Perplexity: 7.4401\n",
      "Epoch [3/3], Step [80/4624], Loss: 2.3002, Perplexity: 9.9759\n",
      "Epoch [3/3], Step [100/4624], Loss: 1.7836, Perplexity: 5.9511\n",
      "Epoch [3/3], Step [120/4624], Loss: 1.9570, Perplexity: 7.0778\n",
      "Epoch [3/3], Step [140/4624], Loss: 1.9610, Perplexity: 7.1061\n",
      "Epoch [3/3], Step [160/4624], Loss: 1.8959, Perplexity: 6.6585\n",
      "Epoch [3/3], Step [180/4624], Loss: 1.8648, Perplexity: 6.4547\n",
      "Epoch [3/3], Step [200/4624], Loss: 1.9073, Perplexity: 6.7351\n",
      "Epoch [3/3], Step [220/4624], Loss: 1.8428, Perplexity: 6.3141\n",
      "Epoch [3/3], Step [240/4624], Loss: 1.9170, Perplexity: 6.8006\n",
      "Epoch [3/3], Step [260/4624], Loss: 1.9261, Perplexity: 6.8624\n",
      "Epoch [3/3], Step [280/4624], Loss: 2.0430, Perplexity: 7.7137\n",
      "Epoch [3/3], Step [300/4624], Loss: 1.8000, Perplexity: 6.0499\n",
      "Epoch [3/3], Step [320/4624], Loss: 2.1782, Perplexity: 8.8303\n",
      "Epoch [3/3], Step [340/4624], Loss: 1.9583, Perplexity: 7.0871\n",
      "Epoch [3/3], Step [360/4624], Loss: 1.9409, Perplexity: 6.9653\n",
      "Epoch [3/3], Step [380/4624], Loss: 2.0844, Perplexity: 8.0399\n",
      "Epoch [3/3], Step [400/4624], Loss: 1.8650, Perplexity: 6.4558\n",
      "Epoch [3/3], Step [420/4624], Loss: 2.2390, Perplexity: 9.3839\n",
      "Epoch [3/3], Step [440/4624], Loss: 1.9670, Perplexity: 7.1493\n",
      "Epoch [3/3], Step [460/4624], Loss: 1.9050, Perplexity: 6.7192\n",
      "Epoch [3/3], Step [480/4624], Loss: 1.8865, Perplexity: 6.5962\n",
      "Epoch [3/3], Step [500/4624], Loss: 1.9475, Perplexity: 7.0113\n",
      "Epoch [3/3], Step [520/4624], Loss: 1.9737, Perplexity: 7.1969\n",
      "Epoch [3/3], Step [540/4624], Loss: 1.9720, Perplexity: 7.1849\n",
      "Epoch [3/3], Step [560/4624], Loss: 2.0517, Perplexity: 7.7811\n",
      "Epoch [3/3], Step [580/4624], Loss: 1.9736, Perplexity: 7.1965\n",
      "Epoch [3/3], Step [600/4624], Loss: 1.8898, Perplexity: 6.6182\n",
      "Epoch [3/3], Step [620/4624], Loss: 1.8994, Perplexity: 6.6821\n",
      "Epoch [3/3], Step [640/4624], Loss: 1.9247, Perplexity: 6.8529\n",
      "Epoch [3/3], Step [660/4624], Loss: 1.8991, Perplexity: 6.6801\n",
      "Epoch [3/3], Step [680/4624], Loss: 1.8804, Perplexity: 6.5559\n",
      "Epoch [3/3], Step [700/4624], Loss: 2.0006, Perplexity: 7.3934\n",
      "Epoch [3/3], Step [720/4624], Loss: 1.9622, Perplexity: 7.1148\n",
      "Epoch [3/3], Step [740/4624], Loss: 2.1668, Perplexity: 8.7301\n",
      "Epoch [3/3], Step [760/4624], Loss: 2.1432, Perplexity: 8.5264\n",
      "Epoch [3/3], Step [780/4624], Loss: 2.0928, Perplexity: 8.1073\n",
      "Epoch [3/3], Step [800/4624], Loss: 1.9599, Perplexity: 7.0985\n",
      "Epoch [3/3], Step [820/4624], Loss: 1.9528, Perplexity: 7.0484\n",
      "Epoch [3/3], Step [840/4624], Loss: 2.0173, Perplexity: 7.5180\n",
      "Epoch [3/3], Step [860/4624], Loss: 1.9638, Perplexity: 7.1265\n",
      "Epoch [3/3], Step [880/4624], Loss: 2.1790, Perplexity: 8.8372\n",
      "Epoch [3/3], Step [900/4624], Loss: 1.8796, Perplexity: 6.5508\n",
      "Epoch [3/3], Step [920/4624], Loss: 2.3422, Perplexity: 10.4046\n",
      "Epoch [3/3], Step [940/4624], Loss: 1.8586, Perplexity: 6.4150\n",
      "Epoch [3/3], Step [960/4624], Loss: 1.8407, Perplexity: 6.3010\n",
      "Epoch [3/3], Step [980/4624], Loss: 1.8857, Perplexity: 6.5908\n",
      "Epoch [3/3], Step [1000/4624], Loss: 1.8450, Perplexity: 6.3284\n",
      "Epoch [3/3], Step [1020/4624], Loss: 1.8148, Perplexity: 6.1401\n",
      "Epoch [3/3], Step [1040/4624], Loss: 1.8953, Perplexity: 6.6545\n",
      "Epoch [3/3], Step [1060/4624], Loss: 1.8269, Perplexity: 6.2145\n",
      "Epoch [3/3], Step [1080/4624], Loss: 1.8725, Perplexity: 6.5047\n",
      "Epoch [3/3], Step [1100/4624], Loss: 1.9894, Perplexity: 7.3113\n",
      "Epoch [3/3], Step [1120/4624], Loss: 1.9681, Perplexity: 7.1570\n",
      "Epoch [3/3], Step [1140/4624], Loss: 1.9698, Perplexity: 7.1691\n",
      "Epoch [3/3], Step [1160/4624], Loss: 1.9764, Perplexity: 7.2170\n",
      "Epoch [3/3], Step [1180/4624], Loss: 1.8671, Perplexity: 6.4698\n",
      "Epoch [3/3], Step [1200/4624], Loss: 2.0008, Perplexity: 7.3953\n",
      "Epoch [3/3], Step [1220/4624], Loss: 1.9417, Perplexity: 6.9708\n",
      "Epoch [3/3], Step [1240/4624], Loss: 2.0873, Perplexity: 8.0628\n",
      "Epoch [3/3], Step [1260/4624], Loss: 2.0118, Perplexity: 7.4771\n",
      "Epoch [3/3], Step [1280/4624], Loss: 2.0393, Perplexity: 7.6854\n",
      "Epoch [3/3], Step [1300/4624], Loss: 2.0155, Perplexity: 7.5044\n",
      "Epoch [3/3], Step [1320/4624], Loss: 2.4024, Perplexity: 11.0495\n",
      "Epoch [3/3], Step [1340/4624], Loss: 1.8715, Perplexity: 6.4979\n",
      "Epoch [3/3], Step [1360/4624], Loss: 2.0362, Perplexity: 7.6614\n",
      "Epoch [3/3], Step [1380/4624], Loss: 1.9959, Perplexity: 7.3588\n",
      "Epoch [3/3], Step [1400/4624], Loss: 2.0276, Perplexity: 7.5961\n",
      "Epoch [3/3], Step [1420/4624], Loss: 1.9498, Perplexity: 7.0275\n",
      "Epoch [3/3], Step [1440/4624], Loss: 1.9395, Perplexity: 6.9550\n",
      "Epoch [3/3], Step [1460/4624], Loss: 1.8346, Perplexity: 6.2624\n",
      "Epoch [3/3], Step [1480/4624], Loss: 1.9342, Perplexity: 6.9186\n",
      "Epoch [3/3], Step [1500/4624], Loss: 1.9252, Perplexity: 6.8564\n",
      "Epoch [3/3], Step [1520/4624], Loss: 2.3290, Perplexity: 10.2681\n",
      "Epoch [3/3], Step [1540/4624], Loss: 2.2812, Perplexity: 9.7880\n",
      "Epoch [3/3], Step [1560/4624], Loss: 2.1076, Perplexity: 8.2282\n",
      "Epoch [3/3], Step [1580/4624], Loss: 1.8199, Perplexity: 6.1714\n",
      "Epoch [3/3], Step [1600/4624], Loss: 1.9481, Perplexity: 7.0155\n",
      "Epoch [3/3], Step [1620/4624], Loss: 1.9224, Perplexity: 6.8371\n",
      "Epoch [3/3], Step [1640/4624], Loss: 1.9341, Perplexity: 6.9178\n",
      "Epoch [3/3], Step [1660/4624], Loss: 1.9889, Perplexity: 7.3077\n",
      "Epoch [3/3], Step [1680/4624], Loss: 2.1695, Perplexity: 8.7535\n",
      "Epoch [3/3], Step [1700/4624], Loss: 2.0583, Perplexity: 7.8328\n",
      "Epoch [3/3], Step [1720/4624], Loss: 2.0135, Perplexity: 7.4894\n",
      "Epoch [3/3], Step [1740/4624], Loss: 2.2004, Perplexity: 9.0282\n",
      "Epoch [3/3], Step [1760/4624], Loss: 1.9346, Perplexity: 6.9216\n",
      "Epoch [3/3], Step [1780/4624], Loss: 1.8358, Perplexity: 6.2700\n",
      "Epoch [3/3], Step [1800/4624], Loss: 1.9103, Perplexity: 6.7554\n",
      "Epoch [3/3], Step [1820/4624], Loss: 1.9787, Perplexity: 7.2336\n",
      "Epoch [3/3], Step [1840/4624], Loss: 1.9746, Perplexity: 7.2035\n",
      "Epoch [3/3], Step [1860/4624], Loss: 2.2715, Perplexity: 9.6936\n",
      "Epoch [3/3], Step [1880/4624], Loss: 2.1670, Perplexity: 8.7320\n",
      "Epoch [3/3], Step [1900/4624], Loss: 1.9821, Perplexity: 7.2577\n",
      "Epoch [3/3], Step [1920/4624], Loss: 1.9193, Perplexity: 6.8163\n",
      "Epoch [3/3], Step [1940/4624], Loss: 2.0067, Perplexity: 7.4389\n",
      "Epoch [3/3], Step [1960/4624], Loss: 1.7763, Perplexity: 5.9078\n",
      "Epoch [3/3], Step [1980/4624], Loss: 1.8647, Perplexity: 6.4539\n",
      "Epoch [3/3], Step [2000/4624], Loss: 1.7917, Perplexity: 5.9998\n",
      "Epoch [3/3], Step [2020/4624], Loss: 1.8966, Perplexity: 6.6632\n",
      "Epoch [3/3], Step [2040/4624], Loss: 2.0253, Perplexity: 7.5786\n",
      "Epoch [3/3], Step [2060/4624], Loss: 1.9153, Perplexity: 6.7890\n",
      "Epoch [3/3], Step [2080/4624], Loss: 2.2946, Perplexity: 9.9207\n",
      "Epoch [3/3], Step [2100/4624], Loss: 1.9832, Perplexity: 7.2658\n",
      "Epoch [3/3], Step [2120/4624], Loss: 1.8156, Perplexity: 6.1451\n",
      "Epoch [3/3], Step [2140/4624], Loss: 2.1580, Perplexity: 8.6538\n",
      "Epoch [3/3], Step [2160/4624], Loss: 2.4303, Perplexity: 11.3626\n",
      "Epoch [3/3], Step [2180/4624], Loss: 1.8891, Perplexity: 6.6132\n",
      "Epoch [3/3], Step [2200/4624], Loss: 1.9328, Perplexity: 6.9090\n",
      "Epoch [3/3], Step [2220/4624], Loss: 1.8630, Perplexity: 6.4430\n",
      "Epoch [3/3], Step [2240/4624], Loss: 2.0476, Perplexity: 7.7489\n",
      "Epoch [3/3], Step [2260/4624], Loss: 2.0671, Perplexity: 7.9017\n",
      "Epoch [3/3], Step [2280/4624], Loss: 1.8544, Perplexity: 6.3879\n",
      "Epoch [3/3], Step [2300/4624], Loss: 1.9375, Perplexity: 6.9416\n",
      "Epoch [3/3], Step [2320/4624], Loss: 1.8211, Perplexity: 6.1785\n",
      "Epoch [3/3], Step [2340/4624], Loss: 2.2678, Perplexity: 9.6580\n",
      "Epoch [3/3], Step [2360/4624], Loss: 2.0458, Perplexity: 7.7351\n",
      "Epoch [3/3], Step [2380/4624], Loss: 2.1083, Perplexity: 8.2345\n",
      "Epoch [3/3], Step [2400/4624], Loss: 2.0812, Perplexity: 8.0141\n",
      "Epoch [3/3], Step [2420/4624], Loss: 2.0609, Perplexity: 7.8528\n",
      "Epoch [3/3], Step [2440/4624], Loss: 1.9111, Perplexity: 6.7602\n",
      "Epoch [3/3], Step [2460/4624], Loss: 2.3143, Perplexity: 10.1177\n",
      "Epoch [3/3], Step [2480/4624], Loss: 1.9041, Perplexity: 6.7132\n",
      "Epoch [3/3], Step [2500/4624], Loss: 2.0150, Perplexity: 7.5006\n",
      "Epoch [3/3], Step [2520/4624], Loss: 2.2713, Perplexity: 9.6917\n",
      "Epoch [3/3], Step [2540/4624], Loss: 1.8089, Perplexity: 6.1040\n",
      "Epoch [3/3], Step [2560/4624], Loss: 1.9200, Perplexity: 6.8213\n",
      "Epoch [3/3], Step [2580/4624], Loss: 1.9285, Perplexity: 6.8789\n",
      "Epoch [3/3], Step [2600/4624], Loss: 2.0735, Perplexity: 7.9524\n",
      "Epoch [3/3], Step [2620/4624], Loss: 1.9836, Perplexity: 7.2688\n",
      "Epoch [3/3], Step [2640/4624], Loss: 1.9791, Perplexity: 7.2360\n",
      "Epoch [3/3], Step [2660/4624], Loss: 2.1275, Perplexity: 8.3941\n",
      "Epoch [3/3], Step [2680/4624], Loss: 1.9817, Perplexity: 7.2554\n",
      "Epoch [3/3], Step [2700/4624], Loss: 1.9170, Perplexity: 6.8004\n",
      "Epoch [3/3], Step [2720/4624], Loss: 1.9152, Perplexity: 6.7886\n",
      "Epoch [3/3], Step [2740/4624], Loss: 1.9794, Perplexity: 7.2386\n",
      "Epoch [3/3], Step [2760/4624], Loss: 1.9255, Perplexity: 6.8583\n",
      "Epoch [3/3], Step [2780/4624], Loss: 2.1953, Perplexity: 8.9826\n",
      "Epoch [3/3], Step [2800/4624], Loss: 1.9085, Perplexity: 6.7433\n",
      "Epoch [3/3], Step [2820/4624], Loss: 1.7716, Perplexity: 5.8804\n",
      "Epoch [3/3], Step [2840/4624], Loss: 1.9747, Perplexity: 7.2042\n",
      "Epoch [3/3], Step [2860/4624], Loss: 1.9443, Perplexity: 6.9889\n",
      "Epoch [3/3], Step [2880/4624], Loss: 2.0060, Perplexity: 7.4332\n",
      "Epoch [3/3], Step [2900/4624], Loss: 2.0089, Perplexity: 7.4550\n",
      "Epoch [3/3], Step [2920/4624], Loss: 1.9731, Perplexity: 7.1931\n",
      "Epoch [3/3], Step [2940/4624], Loss: 2.0163, Perplexity: 7.5107\n",
      "Epoch [3/3], Step [2960/4624], Loss: 2.0320, Perplexity: 7.6294\n",
      "Epoch [3/3], Step [2980/4624], Loss: 2.6503, Perplexity: 14.1581\n",
      "Epoch [3/3], Step [3000/4624], Loss: 1.9642, Perplexity: 7.1289\n",
      "Epoch [3/3], Step [3020/4624], Loss: 2.2286, Perplexity: 9.2870\n",
      "Epoch [3/3], Step [3040/4624], Loss: 1.8836, Perplexity: 6.5775\n",
      "Epoch [3/3], Step [3060/4624], Loss: 2.0844, Perplexity: 8.0401\n",
      "Epoch [3/3], Step [3080/4624], Loss: 1.9030, Perplexity: 6.7060\n",
      "Epoch [3/3], Step [3100/4624], Loss: 2.1143, Perplexity: 8.2835\n",
      "Epoch [3/3], Step [3120/4624], Loss: 1.9151, Perplexity: 6.7874\n",
      "Epoch [3/3], Step [3140/4624], Loss: 2.0209, Perplexity: 7.5448\n",
      "Epoch [3/3], Step [3160/4624], Loss: 1.9406, Perplexity: 6.9632\n",
      "Epoch [3/3], Step [3180/4624], Loss: 2.1295, Perplexity: 8.4110\n",
      "Epoch [3/3], Step [3200/4624], Loss: 1.9047, Perplexity: 6.7174\n",
      "Epoch [3/3], Step [3220/4624], Loss: 1.7936, Perplexity: 6.0112\n",
      "Epoch [3/3], Step [3240/4624], Loss: 1.8926, Perplexity: 6.6366\n",
      "Epoch [3/3], Step [3260/4624], Loss: 1.8259, Perplexity: 6.2086\n",
      "Epoch [3/3], Step [3280/4624], Loss: 1.8447, Perplexity: 6.3264\n",
      "Epoch [3/3], Step [3300/4624], Loss: 1.9737, Perplexity: 7.1975\n",
      "Epoch [3/3], Step [3320/4624], Loss: 1.8524, Perplexity: 6.3750\n",
      "Epoch [3/3], Step [3340/4624], Loss: 1.9695, Perplexity: 7.1672\n",
      "Epoch [3/3], Step [3360/4624], Loss: 1.8706, Perplexity: 6.4921\n",
      "Epoch [3/3], Step [3380/4624], Loss: 1.8494, Perplexity: 6.3558\n",
      "Epoch [3/3], Step [3400/4624], Loss: 1.9413, Perplexity: 6.9678\n",
      "Epoch [3/3], Step [3420/4624], Loss: 1.8533, Perplexity: 6.3808\n",
      "Epoch [3/3], Step [3440/4624], Loss: 1.9016, Perplexity: 6.6963\n",
      "Epoch [3/3], Step [3460/4624], Loss: 1.9719, Perplexity: 7.1843\n",
      "Epoch [3/3], Step [3480/4624], Loss: 1.9940, Perplexity: 7.3447\n",
      "Epoch [3/3], Step [3500/4624], Loss: 1.9026, Perplexity: 6.7032\n",
      "Epoch [3/3], Step [3520/4624], Loss: 2.1296, Perplexity: 8.4114\n",
      "Epoch [3/3], Step [3540/4624], Loss: 1.9718, Perplexity: 7.1835\n",
      "Epoch [3/3], Step [3560/4624], Loss: 1.8824, Perplexity: 6.5694\n",
      "Epoch [3/3], Step [3580/4624], Loss: 1.7470, Perplexity: 5.7376\n",
      "Epoch [3/3], Step [3600/4624], Loss: 1.9529, Perplexity: 7.0490\n",
      "Epoch [3/3], Step [3620/4624], Loss: 1.9396, Perplexity: 6.9557\n",
      "Epoch [3/3], Step [3640/4624], Loss: 1.9361, Perplexity: 6.9316\n",
      "Epoch [3/3], Step [3660/4624], Loss: 1.9472, Perplexity: 7.0087\n",
      "Epoch [3/3], Step [3680/4624], Loss: 1.9235, Perplexity: 6.8450\n",
      "Epoch [3/3], Step [3700/4624], Loss: 2.1081, Perplexity: 8.2326\n",
      "Epoch [3/3], Step [3720/4624], Loss: 1.7792, Perplexity: 5.9251\n",
      "Epoch [3/3], Step [3740/4624], Loss: 2.0036, Perplexity: 7.4161\n",
      "Epoch [3/3], Step [3760/4624], Loss: 1.9462, Perplexity: 7.0023\n",
      "Epoch [3/3], Step [3780/4624], Loss: 1.7493, Perplexity: 5.7505\n",
      "Epoch [3/3], Step [3800/4624], Loss: 2.1980, Perplexity: 9.0067\n",
      "Epoch [3/3], Step [3820/4624], Loss: 1.7594, Perplexity: 5.8091\n",
      "Epoch [3/3], Step [3840/4624], Loss: 2.2602, Perplexity: 9.5847\n",
      "Epoch [3/3], Step [3860/4624], Loss: 1.8460, Perplexity: 6.3344\n",
      "Epoch [3/3], Step [3880/4624], Loss: 1.8856, Perplexity: 6.5905\n",
      "Epoch [3/3], Step [3900/4624], Loss: 1.9188, Perplexity: 6.8131\n",
      "Epoch [3/3], Step [3920/4624], Loss: 1.9865, Perplexity: 7.2899\n",
      "Epoch [3/3], Step [3940/4624], Loss: 1.8619, Perplexity: 6.4359\n",
      "Epoch [3/3], Step [3960/4624], Loss: 1.9195, Perplexity: 6.8178\n",
      "Epoch [3/3], Step [3980/4624], Loss: 2.1630, Perplexity: 8.6973\n",
      "Epoch [3/3], Step [4000/4624], Loss: 1.8622, Perplexity: 6.4378\n",
      "Epoch [3/3], Step [4020/4624], Loss: 2.0446, Perplexity: 7.7265\n",
      "Epoch [3/3], Step [4040/4624], Loss: 1.9677, Perplexity: 7.1541\n",
      "Epoch [3/3], Step [4060/4624], Loss: 1.9229, Perplexity: 6.8408\n",
      "Epoch [3/3], Step [4080/4624], Loss: 1.7950, Perplexity: 6.0197\n",
      "Epoch [3/3], Step [4100/4624], Loss: 2.0052, Perplexity: 7.4274\n",
      "Epoch [3/3], Step [4120/4624], Loss: 2.0326, Perplexity: 7.6341\n",
      "Epoch [3/3], Step [4140/4624], Loss: 1.9200, Perplexity: 6.8207\n",
      "Epoch [3/3], Step [4160/4624], Loss: 2.0917, Perplexity: 8.0988\n",
      "Epoch [3/3], Step [4180/4624], Loss: 2.0955, Perplexity: 8.1295\n",
      "Epoch [3/3], Step [4200/4624], Loss: 1.8950, Perplexity: 6.6524\n",
      "Epoch [3/3], Step [4220/4624], Loss: 2.4914, Perplexity: 12.0778\n",
      "Epoch [3/3], Step [4240/4624], Loss: 1.9283, Perplexity: 6.8776\n",
      "Epoch [3/3], Step [4260/4624], Loss: 1.8081, Perplexity: 6.0988\n",
      "Epoch [3/3], Step [4280/4624], Loss: 2.4428, Perplexity: 11.5048\n",
      "Epoch [3/3], Step [4300/4624], Loss: 1.7867, Perplexity: 5.9699\n",
      "Epoch [3/3], Step [4320/4624], Loss: 2.0471, Perplexity: 7.7456\n",
      "Epoch [3/3], Step [4340/4624], Loss: 2.2216, Perplexity: 9.2216\n",
      "Epoch [3/3], Step [4360/4624], Loss: 1.7999, Perplexity: 6.0488\n",
      "Epoch [3/3], Step [4380/4624], Loss: 1.8971, Perplexity: 6.6667\n",
      "Epoch [3/3], Step [4400/4624], Loss: 1.9092, Perplexity: 6.7479\n",
      "Epoch [3/3], Step [4420/4624], Loss: 1.8926, Perplexity: 6.6365\n",
      "Epoch [3/3], Step [4440/4624], Loss: 1.9200, Perplexity: 6.8209\n",
      "Epoch [3/3], Step [4460/4624], Loss: 1.8845, Perplexity: 6.5832\n",
      "Epoch [3/3], Step [4480/4624], Loss: 1.8453, Perplexity: 6.3303\n",
      "Epoch [3/3], Step [4500/4624], Loss: 1.9084, Perplexity: 6.7421\n",
      "Epoch [3/3], Step [4520/4624], Loss: 1.8581, Perplexity: 6.4118\n",
      "Epoch [3/3], Step [4540/4624], Loss: 1.8680, Perplexity: 6.4754\n",
      "Epoch [3/3], Step [4560/4624], Loss: 1.8200, Perplexity: 6.1717\n",
      "Epoch [3/3], Step [4580/4624], Loss: 1.9462, Perplexity: 7.0022\n",
      "Epoch [3/3], Step [4600/4624], Loss: 1.8629, Perplexity: 6.4425\n",
      "Epoch [3/3], Step [4620/4624], Loss: 1.9593, Perplexity: 7.0943\n"
     ]
    }
   ],
   "source": [
    "f = open(log_file, \"w\")\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for i_step in range(1, total_step+1):\n",
    "\n",
    "        #Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        #Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        #Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "        \n",
    "        #Move batch of images and captions to GPU\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        #passing the inputs through the CNN-RNN model\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        #Calculating the batch Loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "        #Backwarding pass\n",
    "\n",
    "        loss.backward()\n",
    "        #updating the parameters in the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        #Getting training statistics\n",
    "        stats = (\n",
    "            f\"Epoch [{epoch}/{num_epochs}], Step [{i_step}/{total_step}], \"\n",
    "            f\"Loss: {loss.item():.4f}, Perplexity: {np.exp(loss.item()):.4f}\"\n",
    "        )\n",
    "\n",
    "        #printing training statistics to file\n",
    "        f.write(stats + \"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        #Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print(\"\\r\" + stats)\n",
    "    #save the weights\n",
    "    if epoch% save_every == 0:\n",
    "        torch.save(\n",
    "            decoder.state_dict(), os.path.join(\"models\", \"decoder-%d.pkl\" % epoch)\n",
    "        )\n",
    "        torch.save(\n",
    "            encoder.state_dict(), os.path.join(\"models\", \"encoder-%d.pkl\" %epoch)\n",
    "        )\n",
    "#close the training log file.\n",
    "f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc1dbcdf-ef1c-4bdd-8721-e941c77cdc4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T18:01:23.449394Z",
     "start_time": "2024-08-03T18:01:23.371460Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-03T18:01:23.451964Z",
     "start_time": "2024-08-03T18:01:23.374687Z"
    }
   },
   "id": "b04306e0b63d10bb"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-03T18:01:23.452123Z",
     "start_time": "2024-08-03T18:01:23.375176Z"
    }
   },
   "id": "8bf0b956fe9660a0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-03T18:01:23.453489Z",
     "start_time": "2024-08-03T18:01:23.375492Z"
    }
   },
   "id": "1f6228b1cd84795c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-03T18:01:23.453722Z",
     "start_time": "2024-08-03T18:01:23.376016Z"
    }
   },
   "id": "e2022d7dcb666090"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4c29addcfe45372a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
